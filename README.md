# Project-3

[Prosjekt3](https://compphysics.github.io/MachineLearning/doc/Projects/2021/Project3/pdf/Project3.pdf)

[Bankruptcy](https://www.kaggle.com/fedesoriano/company-bankruptcy-prediction)

[Overleaf](https://www.overleaf.com/project/6167f4a28b15e3ffa0aab71c)

Finn eget dataset og driv forskning med, eller bruk
1. Kaggle
2. https://archive.ics.uci.edu/ml/index.php
3. Andre kilder(Altså ka en vill?)

# Raport mal
1. Strukturer og les dataen
2. Inkluder 2 central algoritmer eller utforsk decision tree til bagging, random forest og boosting.
3. Beskriv algoritmen og dens implementasjon, gjennomfør tester.
4. Vis fram resultatet. Hvis det finnes noe resultat fra andre, så link det også
5. Badabim badabom

Vi må inkludere minst 2 av følgende metoder
- Linear Regression (Inkludert Ridge og Lasso)
- Logistic Regression
- Neural Netowork
- Convolution Neural Netowkrs
- Recurrent Neural Networks
- Adversarial Neural Netowrks
- Support Vector Machines (SVM)
- Descision Trees
- Random Forest
- Bagging
- Boosting

## Eks
Utforske alle mulighetene for Descision trees via Bagging og Voting Classifiers, til Random Forest, bootsing og tilslutt XGboost.

# Oppgaven!

1. Utfør bias-variance tradeoff ved å bruke minst 3 av hovedalgorithmene vi har diskutert i kurset. Vi bruker et classification problem.
2. Vi bruker bootstrap for resampling for å få best mulig estimat.
3. Metodene vi studerer er
    1. Linear Regression (OLS, Ridge og Lasso)
    2. Deep learning (Feed forward neural networks og recurrent neural netowrks)
    3. Ensemble methods (decision trees, bagging, random forests og boosting)
    4. Support vector machines (SVM)
4. Studer bias-variance tradeoff på minst 3 av disse settene av algoritmer for dataen våres utifra dens kompleksitet. Kommenter og forklar resultatet. Forklar de ulike positive og negative sidene. Er det noe som gir lav variance og lav bias?
5. HINT! Når en bruker ulike metoder, vær oppmerksom på hvordan du representerer kompleksiteten av modellen.

<!-- # Oppgaven

1. Skriv kode SVM'er og/ekker Decision trees/Random forest/Bagging/Boosting eller bruk tilgjenlige funksjonaliteter scikit-learn, tensorflow, etc.
2. Inkluder estimatene fra prosjekt 1 og 2, altså R2, MSE, confusion matrix, accuracy score, infromation gain, ROC og Cumulative gains kurver og andre relevante. Cross-validation og/Eller bootstrap hvis en trenger
3. Utforsk de ulike aktiverings funksjonene i deep learning og ulike tilnærminger til Stochastic Gradient Descent.
4. Hvis mulig, knytt datasettene opp mot eksisterenede research og analyser derifra. -->

## Litt ting
Denne innleveringen handler om å bruke Neural Networks 